# Session 4: Quiz

## 1. Code Understanding
In the model definition, why do we have `self.activation = nn.Tanh()` between the hidden layer and output layer? What would happen if we removed all activation functions?

## 2. Loss Interpretation
If the model's loss is 2.3 at the start and drops to 0.5 after training, what does this tell us about the model's learning?

## 3. Embedding Dimension
Why might we choose different embedding dimensions (like 16 vs 256 vs 512) for different problems?

## 4. Practical Exercise
Modify the code to use a larger hidden layer (e.g., 32 instead of 16). Does this affect training? Why or why not for this tiny dataset?

## 5. Connection to Bigrams
Compare the neural model's predictions to what a pure bigram count model would give. Are they similar? When might they differ?

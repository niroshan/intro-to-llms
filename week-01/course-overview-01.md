#Week 1: Introduction to Language Models and N-Gram Text Generation

**Goal**: Grasp what a language model is and build a simple text generator using basic probabilistic modeling (no neural networks yet). This establishes foundational understanding of how predicting the next word/character in a sequence works.

**Session 1**: Learn what Language Models do. Start with a plain-language explanation: a language model is an AI system that estimates the probability of a token (word or character) or sequence of tokens appearing in a given context. In practice, this means given some text, the model predicts what comes next – enabling applications like text generation, translation, or summarization. Discuss examples (e.g. how your phone suggests next words). Introduce the concept of context: the idea that surrounding words help predict the next word. Emphasize that early language models used N-grams – sequences of N words – to capture context. (For example, a bigram model looks at the previous 1 word; a trigram looks at the previous 2 words, etc.) Longer N-grams give more context but suffer from sparsity (many possible sequences are never seen in training).

**Session 2**: Build a simple N-gram text generator. Choose a small text corpus (e.g. a few chapters of a public-domain book or a Wikipedia article). Use Python to construct an N-gram frequency table: for simplicity, start with bigrams (N=2). This means counting how often each word follows each other word in the corpus. Then use this table to generate random text: pick a start word and repeatedly sample the next word based on the learned probabilities. (For example, if “the” is followed by “cat” 50% of the time and “dog” 50% in your data, then after “the” your model will choose “cat” or “dog” with equal probability.) Observe the output – it will be somewhat grammatical in short phrases but likely loses coherence beyond a few words, due to the limited context length. Discuss limitations: the bigram model doesn’t remember anything beyond one word of context, causing inconsistent or nonsensical longer sentences. This exercise illustrates the basic idea of a language model (predicting next tokens) and why we need more powerful methods to handle longer context and generalize better (N-gram models face data sparsity and can’t capture long-term structure). The takeaway: Next week, we’ll begin designing a model that “learns” patterns from data rather than relying only on observed word frequencies.

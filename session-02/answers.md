# Session 2: Quiz Answers

## 1. Probability Practice
Using the rhyme corpus example, what is P(Jill|and) according to the model? (Use counts from the corpus: "and Jill" vs total "and ...")

**Answer:** In the corpus, "and Jill" appears 1 time ("and Jill came tumbling after."), "and all" appears 2 times ("and all the king's horses", "and all the king's men"), "and broke" appears 1 time ("and broke his crown"). So total "and" occurrences = 4 (assuming 5 from code was including maybe another "and"? Actually, double-check: the code said 5 for 'and'. Possibly it counted "and" 5 times including one at start of a sentence or after comma. Let's stick with these four main occurrences). So P(Jill|and) ≈ 1/4 = 0.25 or 25%. (The code output said 0.20, implying maybe 1/5, but we'll accept ~0.2-0.25 range with reasoning.)

## 2. Understanding Output
The generated sentence in the example was *"jack fell down and all the king's horses and jill came tumbling after"*. Identify two points in that sentence where the model likely *stitched together* pieces from different parts of the training data.

**Answer:** One stitch is after "fell down and all the king's horses" – in Jack and Jill, "fell down and broke his crown," but our sentence went to "and all the king's horses" which is from Humpty Dumpty. Another stitch is after "horses" – in Humpty Dumpty the line ends with "horses and all the king's men <END>", but our model instead went to "and jill came tumbling after", jumping back to Jack and Jill's ending. These points ("...fell down and **all the king's horses**..." and then "...horses **and jill** came...") show the merging of two rhymes. The model did this because "and" can be followed by both "broke" (Jack rhyme) or "all" (Humpty rhyme), and it happened to pick "all"; then "horses" was typically end-of-sentence in training, so it picked "and Jill" which often starts after a comma in the Jack rhyme. It's mixing contexts due to only local awareness.

## 3. Modification Task
How might you modify the generation process to ensure the model doesn't produce an output that's too long or repetitive (like potentially infinite loops)? Describe one method and why it would help.

**Answer:** One method: implement a maximum length or stop if a loop is detected. We already use a maximum word count (like 20 words) to avoid extremely long outputs. Another method is to stop generation if an `<END>` token is produced, ensuring we end at a sentence boundary. We did include that, which stops sentences. To avoid repetition loops (like cycling "and the and the…"), one could add a rule that if the model repeats a bigram that it has just produced (or if a sequence of last few words repeats), then break out or force an end. A simpler method: incorporate randomness but also perhaps avoid picking a word with probability below a threshold if it was just used (though that breaks the Markov assumption a bit). In summary, the easiest answer: *limit the length* to prevent infinite run-on, and *stop at end-of-sentence tokens* to get coherent breaks, both of which we have done. In advanced cases, strategies like **beam search** or **temperature sampling** can reduce nonsense repetition (but that's beyond the current model's simplicity).

## 4. Critical Thinking
If we increased N to 3 (trigram model) for the same tiny nursery rhyme corpus, do you think the generated sentences would be better, worse, or just different? Why?

**Answer:** Likely better in the sense of staying true to original sentences (because trigram would capture longer fragments like "Jack and Jill" -> "went" as a triple, etc., so it might recite the rhymes almost verbatim since the corpus is small). In a small corpus, a trigram model might end up memorizing whole sentences since there are not many variations. Generally, trigram generation tends to produce slightly more coherent sentences than bigram because it considers a broader context (two words). It would be less likely to do the weird stitch mid-sentence that our bigram did, since it would know "down and all" vs "down and broke" are distinguished by two-word context "fell down and". However, with such limited data, it might also severely overfit (just regurgitate exact lines). So the sentences might end up *correct* but not very creative. With a larger corpus, trigram would indeed be an improvement in coherence over bigram (less nonsensical transitions). The drawback is it needs more data to avoid holes. In our small case, it might not even have any holes because it saw all combinations needed.

## 5. Brainstorm
Based on what you've learned, list one real application where N-gram models might still be useful today despite more advanced models being available, and explain why.

**Answer:** N-gram models can still be useful in scenarios where simplicity and speed are key and the text patterns are fairly predictable. One example is **autocomplete for a specific domain** (like code editors suggesting code completions, where a trigram model might be trained on lots of code to suggest the next token). In such cases, an n-gram model can be efficient and require less computational resources than a huge neural model, and if the domain is narrow, it might perform quite well. Another example: **spell-checking** or **error detection**, where a simple bigram model might flag unlikely word pairs (like "to the the house") as a probable typo (since "the the" has low probability). Also, **compression algorithms** and some **speech recognition systems** historically used N-grams to predict the next word for efficiency. They remain useful as back-offs or in low-resource environments where a full LLM is impractical.

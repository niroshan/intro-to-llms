# Session 2: Quiz

## 1. Probability Practice
Using the rhyme corpus example, what is P(Jill|and) according to the model? (Use counts from the corpus: "and Jill" vs total "and ...")

## 2. Understanding Output
The generated sentence in the example was *"jack fell down and all the king's horses and jill came tumbling after"*. Identify two points in that sentence where the model likely *stitched together* pieces from different parts of the training data.

## 3. Modification Task
How might you modify the generation process to ensure the model doesn't produce an output that's too long or repetitive (like potentially infinite loops)? Describe one method and why it would help.

## 4. Critical Thinking
If we increased N to 3 (trigram model) for the same tiny nursery rhyme corpus, do you think the generated sentences would be better, worse, or just different? Why?

## 5. Brainstorm
Based on what you've learned, list one real application where N-gram models might still be useful today despite more advanced models being available, and explain why.
